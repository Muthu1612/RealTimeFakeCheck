{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046a4bee-fb2d-4f0c-8483-be0d19e12cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data directory: D:\\florida_coursework\\third_sem\\multimedia_expert_systems\\multimedia_prototype\\data\\audio\\for-norm\\for-norm\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load paths from environment variables\n",
    "data_root = Path(os.getenv(\"DATA_ROOT\", \"./data\"))  # default fallback to ./data\n",
    "image_data_path = \"audio/for-norm/for-norm/\"\n",
    "data_root = data_root/ image_data_path\n",
    "\n",
    "print(f\"Data directory: {data_root}\")\n",
    "print(f\"Exists: {data_root.exists()}\")\n",
    "\n",
    "# Add src to path\n",
    "src_path = Path(r\"D:\\florida_coursework\\third_sem\\multimedia_expert_systems\\multimedia_prototype\\mutlimedia_mvp\\src\")\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from loaders.visual_loader import CelebDFVisualDataset\n",
    "from encoders.visual_encoder import VisualEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abf79da-04b8-4dd7-99d2-2be3f8d30303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395593c9447c46ee89620fc4cf4f421e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/53868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe99ed0b2cd400ca256ee2981a03623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/10798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba359cf6f3a04e498d37372df9d31ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/4634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cfdff3571b4e43a79491a14863a3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/53868 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6036842c1c474798a5c813d57987f369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums:  69%|######8   | 36998/53868 [00:05<00:02, 7398.20it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7153282f7b4fac94677b3009e1f93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/10798 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cb27696fdf44a9b686b1b539c3b612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/4634 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff616bf14014a3d9242882752a623e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c14c6c2b25b4243b35c4d29dbfb5a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddded648a5942d8a282bd2069e0d4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "import gc\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "import torchaudio  # torchaudio==2.9.0  torch==2.9.0 torchcodec==0.8\n",
    "import sys\n",
    "import datasets  # pip install datasets==3.6.0\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "full_dataset = load_dataset(\n",
    "    \"audiofolder\",\n",
    "    data_dir=data_root,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96486035-2da1-40c8-8198-6343a17feaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 53868\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 10798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 4634\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c35a27-a624-441f-9e04-c03dede35375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muthu\\miniconda3\\envs\\multi_env\\Lib\\site-packages\\transformers\\configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "encoder = Wav2Vec2Model.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173ee8b7-6d1f-49da-8295-ce51832c2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 5.0  # seconds (you can change this)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c4b59-9385-4eaf-95e2-d26f0a00cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DatasetDict\n",
    "# test_only = DatasetDict({\n",
    "#     \"test\": full_dataset[\"test\"]\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe4a65-2ea8-4c75-869d-d67effba501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"audio\",\"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2d7ef-d057-4e56-b274-ba4bb3625bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    processed_dataset[\"train\"],\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator  # <-- important!\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    processed_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada953b3-860c-4c86-a6f9-2801b1d64e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AudioEmbeddingModel(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        if hasattr(self.encoder, \"freeze_feature_extractor\"):\n",
    "            self.encoder.freeze_feature_extractor()\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        outputs = self.encoder(\n",
    "            input_values=input_values,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state  # (B, T_hidden, H)\n",
    "        \n",
    "        # HuggingFace Wav2Vec2 / WavLM already applies attention masking internally\n",
    "        # So you can just do mean pooling over time dimension\n",
    "        embedding = hidden_states.mean(dim=1)     # (B, H)\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6767f8-349a-47b4-95d4-95a92eb56e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_model = AudioEmbeddingModel(encoder).to(device)\n",
    "embedding_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f22f90-4e04-4eea-ab46-e744b146bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio_embs = []\n",
    "all_labels = []\n",
    "\n",
    "embedding_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"]  # still available as tensor\n",
    "\n",
    "        emb = embedding_model(input_values, attention_mask)\n",
    "        all_audio_embs.append(emb.cpu())\n",
    "        all_labels.append(labels)  # collect labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84af38-418d-49ee-b75d-ef985683225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio_embs = torch.cat(all_audio_embs, dim=0)   # (N, H)\n",
    "all_labels = torch.cat(all_labels, dim=0)           # (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837955cc-374b-43ea-a380-c296188b055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"embeddings/audio_embeddings.pt\"\n",
    "torch.save({\n",
    "    \"embeddings\": all_audio_embs,   # (N, D)\n",
    "    \"labels\": all_labels            # (N,)\n",
    "}, save_path)\n",
    "\n",
    "print(\"Saved:\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
